\fancyhead[R]{Leonie Boland}
\subsection{Imitation-based RL Agent}

In conclusion, our attempt to develop an agent that initially imitated the rule-based agent and then enhance its capabilities using Q-learning led 
to mixed results. The training to imitate the rule-based agent was more or less successful, and the agent demonstrated promising performance, aligning with the rule-based 
strategies in a certain point. However, when we introduced Q-learning to further improve its gameplay, the results were disappointing. Instead of enhancing its skills, 
the agent's performance steadily declined during training, reaching a point where it was unable to play the game proficiently.

We hypothesized that a significant factor contributing to this issue was the disparity in the dimensionality of the training sets 
used for imitation and Q-learning. The transition from the well-structured imitation training data to the Q-learning dataset, which likely 
had different dimensions and characteristics, might have caused confusion and hindered the agent's learning process. This mismatch in the training sets 
could have led to the deterioration in performance, highlighting the importance of consistent and compatible data when implementing reinforcement learning techniques. 
Further investigations and refinements in the training process are necessary to address these challenges and achieve successful integration of imitation learning and
Q-learning for optimal agent performance in the game.

\subsection{Outlook}

\subsubsection{Deep Q-Learning Agent}

The Deep Q-learning method has proved itself to be a powerful reinforcement learning technique to create a well playing bomberman agent. Through a series of experiments and analyses specified in \ref{sec:deepqexperiments} we took actions in respect of improving on the basic framework of our model. Despite the promising results, there are a few challenges that remained and could be embarked upon when investing more time. \\

The most obvious flaw of our agent is that it can get stuck in a loop, for example just moving up and down, even though there are many better moves to do. We doubt that this could simply be corrected by more training as we also observed that the rule based agent with a restricted vision has similar problems. There are at least three ways to improve the model in regards to this challenge. First, we could give the agent a larger vision field, which would probably lead to a longer computation time, as the neural network would get more information as input. On the other hand more work could be done on the outside map to give it more relevance in the case that there are no important items left in the agents vision field. And lastly, the agent could start moving in a random way without dropping bombs, to explore the rest of the field. As soon as the agent "sees" a coin, crate or opponent it should act appropriate to that scenario again.

Furthermore the computation of the features given to the neural network is rather minimalist. We could take advantage of what we know from the game state. For example if the agent is in the explosion radius of a bomb we could feed the network with that information, so that it can learn more easily that it needs to move away. However, if going too far with this approach the agent more or less could end up as a rule based agent rather than a reinforcement learning agent. This is a fine line but our implementation could be improved in that sense without making rule based decisions.\\ 

Although the auxiliary rewards have been a very present topic during the development already they still need to be mentioned as a possibility to further enhance the DQN agent. To perfect the rewards one would need to act out even more scenarios in theory and think about how to reward the events in comparison to each other so that the agent can rely on these rewards. Especially in regards to the movement of the agent, which is only rewarded in terms of the distance to the coin, more fine tuning is needed. In the classic environment with a lot of crates and only a few coins it is not unlikely that there is no coin on the field at all. Therefore, the agent should be triggered to move in a promising direction not only because of a coin but also due to crates or opponents. This is only one concrete example but spending even more time to figure out the auxiliary rewards would probably be advisable to reach a strong Bomberman agent model. \\

We went with a very basic neural network architecture only including one convolution and one hidden linear layer for each feature map before concatenating them into one network, which is followed by one hidden linear layer again. As there is no guideline how to create the most efficient neural network for one specific task it is a common practice to test different neural network structures and parameters in the different layers. We chose to have parallel computed convolution layers for the different feature maps but we could have taken all 7 $\times$ 7 feature maps as input to one convolution layer with more channels, in our case 3 channels. This probably would give a similar result to our original computation but is considered to be more efficient computing. However we neglected to experiment with the number of channels or with adding more convolution layers stringed together. Without testing different features in the network's architecture like the number of layers or channels we cannot be sure that we have found the best neural network to solve the problem at hand.

Experimenting with more sophisticated neural network features, for example attention mechanisms, can help to capture long-term dependencies within the Bomberman game. Currently attention mechanisms are most popular in language model, but it could underline the importance of certain actions in a specific environment while suppressing irrelevant information. This might be hard to integrate into our current model, thus probably asks for a new approach, at least in regards to the neural network.

%- input besser computen, vorcomputen was der ideale weg ist, nicht nur sicht geben, state to feature\\
%- rewards noch besser, mit einander matchen\\
%- gefangen in loop, bearbeiten, outside map, rule based agent hat auch probleme, sobald keine items rumlaufen bis wieder was ins sichtfeld kommt, integrieren\\
%- architecture change, nicht parallele convolutions, sondern mit channels, more efficient computing, generell mehr channels

%\subsection{Game Setup}

%The game setup is very well elaborated for our task.

\subsubsection{Imitation-based RL Agent}

The biggest problem with our approach was that the agent kept getting worse during deep Q-Learning, 
even though it performed quite well through training with the rule-based agent. However, we do not want 
to abandon this approach in the future. It needs to be analyzed what the reasons for this were.
It might be beneficial to mix the two training approaches instead of keeping them separate.