\contentsline {section}{\numberline {1}Introduction}{1}{section.1}%
\contentsline {subsection}{\numberline {1.1}Related Work}{2}{subsection.1.1}%
\contentsline {section}{\numberline {2}Fundamentals}{3}{section.2}%
\contentsline {subsection}{\numberline {2.1}Q-Learning}{3}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Q-Learning}{3}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Deep Q-Networks}{3}{subsubsection.2.1.2}%
\contentsline {subsection}{\numberline {2.2}Neural Networks}{4}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Basic Structure of a Neural Network}{4}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Working Principle of Neurons}{4}{subsubsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.3}Training a Neural Network}{4}{subsubsection.2.2.3}%
\contentsline {subsubsection}{\numberline {2.2.4}Batch Normalization}{5}{subsubsection.2.2.4}%
\contentsline {subsection}{\numberline {2.3}Loss Functions}{5}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Cross-Entropy Loss}{5}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Huber Loss}{6}{subsubsection.2.3.2}%
\contentsline {subsubsection}{\numberline {2.3.3}Mean Squared Error (MSE) Loss}{6}{subsubsection.2.3.3}%
\contentsline {subsubsection}{\numberline {2.3.4}Smooth L1 Loss}{6}{subsubsection.2.3.4}%
\contentsline {subsubsection}{\numberline {2.3.5}Optimal Loss Function for Bomberman}{6}{subsubsection.2.3.5}%
\contentsline {subsection}{\numberline {2.4}Optimizer}{7}{subsection.2.4}%
\contentsline {subsubsection}{\numberline {2.4.1}RMSprop}{7}{subsubsection.2.4.1}%
\contentsline {subsubsection}{\numberline {2.4.2}AdamW}{7}{subsubsection.2.4.2}%
\contentsline {subsubsection}{\numberline {2.4.3}Choosing the Right Optimizer}{8}{subsubsection.2.4.3}%
\contentsline {section}{\numberline {3}Methods}{9}{section.3}%
\contentsline {subsection}{\numberline {3.1}Deep Q-Learning Agent}{9}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Data Collection}{9}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}State Representation}{9}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}Model Architecture}{10}{subsubsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.4}Loss Function and Optimizer}{11}{subsubsection.3.1.4}%
\contentsline {subsection}{\numberline {3.2}Imitation-based RL Agent}{12}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Data Collection}{12}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}State Representation}{13}{subsubsection.3.2.2}%
\contentsline {subsubsection}{\numberline {3.2.3}Model Architecture}{16}{subsubsection.3.2.3}%
\contentsline {subsubsection}{\numberline {3.2.4}Loss Function and Optimizer}{16}{subsubsection.3.2.4}%
\contentsline {subsection}{\numberline {3.3}Other Approaches}{17}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}Q-Tables}{17}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}Coin-Collector Agent that sees only one coin}{18}{subsubsection.3.3.2}%
\contentsline {section}{\numberline {4}Training}{19}{section.4}%
\contentsline {subsection}{\numberline {4.1}Deep Q-Learning Agent}{19}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Training Environment Progression}{19}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}Self-Play Strategy}{19}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Auxiliary Rewards}{19}{subsubsection.4.1.3}%
\contentsline {subsection}{\numberline {4.2}Imitation-based RL Agent}{23}{subsection.4.2}%
\contentsline {section}{\numberline {5}Experiments and Results}{25}{section.5}%
\contentsline {subsection}{\numberline {5.1}Deep Q-Learning Agent}{25}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Imitation-based RL Agent}{30}{subsection.5.2}%
\contentsline {section}{\numberline {6}Conclusion}{32}{section.6}%
\contentsline {subsection}{\numberline {6.1}Deep Q-Learning Agent}{32}{subsection.6.1}%
\contentsline {subsubsection}{\numberline {6.1.1}Outlook}{32}{subsubsection.6.1.1}%
\contentsline {subsection}{\numberline {6.2}Imitation-based RL Agent}{34}{subsection.6.2}%
\contentsline {subsubsection}{\numberline {6.2.1}Outlook}{34}{subsubsection.6.2.1}%
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
