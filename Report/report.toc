\contentsline {section}{\numberline {1}Introduction}{1}{section.1}%
\contentsline {subsection}{\numberline {1.1}Related Work}{2}{subsection.1.1}%
\contentsline {section}{\numberline {2}Fundamentals}{3}{section.2}%
\contentsline {subsection}{\numberline {2.1}Q-learning}{3}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Q-Learning}{3}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Deep Q-Networks}{3}{subsubsection.2.1.2}%
\contentsline {subsection}{\numberline {2.2}Neural Networks}{4}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Basic Structure of a Neural Network}{4}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Working Principle of Neurons}{4}{subsubsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.3}Training a Neural Network}{4}{subsubsection.2.2.3}%
\contentsline {subsubsection}{\numberline {2.2.4}Batch Normalization}{5}{subsubsection.2.2.4}%
\contentsline {subsection}{\numberline {2.3}Optimizer}{5}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}RMSprop}{5}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}AdamW}{6}{subsubsection.2.3.2}%
\contentsline {subsubsection}{\numberline {2.3.3}Choosing the Right Optimizer}{7}{subsubsection.2.3.3}%
\contentsline {subsection}{\numberline {2.4}Loss Functions}{7}{subsection.2.4}%
\contentsline {subsubsection}{\numberline {2.4.1}Cross-Entropy Loss}{7}{subsubsection.2.4.1}%
\contentsline {subsubsection}{\numberline {2.4.2}Huber Loss}{7}{subsubsection.2.4.2}%
\contentsline {subsubsection}{\numberline {2.4.3}Mean Squared Error (MSE) Loss}{7}{subsubsection.2.4.3}%
\contentsline {subsubsection}{\numberline {2.4.4}Smooth L1 Loss}{8}{subsubsection.2.4.4}%
\contentsline {subsubsection}{\numberline {2.4.5}Optimal Loss Function for Bomberman}{8}{subsubsection.2.4.5}%
\contentsline {section}{\numberline {3}Methods}{8}{section.3}%
\contentsline {subsection}{\numberline {3.1}Deep Q-Learning Agent}{8}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Data Collection}{9}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}State Representation}{9}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}Model Architecture}{10}{subsubsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.4}Loss Function and Optimizer}{11}{subsubsection.3.1.4}%
\contentsline {subsection}{\numberline {3.2}Imitation-based RL Agent}{12}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Data Collection}{12}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}State Representation}{13}{subsubsection.3.2.2}%
\contentsline {subsubsection}{\numberline {3.2.3}Model Architecture}{16}{subsubsection.3.2.3}%
\contentsline {subsubsection}{\numberline {3.2.4}Loss function and optimizer}{16}{subsubsection.3.2.4}%
\contentsline {subsection}{\numberline {3.3}Other aproaches}{17}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}Q-Tables}{17}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}Coin-Collector Agent that sees only one coin}{18}{subsubsection.3.3.2}%
\contentsline {section}{\numberline {4}Training}{19}{section.4}%
\contentsline {subsection}{\numberline {4.1}Deep Q-Learning Agent}{19}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Training Environment Progression}{19}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}Self-Play Strategy}{19}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Auxiliary Rewards}{19}{subsubsection.4.1.3}%
\contentsline {subsection}{\numberline {4.2}Imitation-based RL Agent}{22}{subsection.4.2}%
\contentsline {section}{\numberline {5}Experiments and Results}{24}{section.5}%
\contentsline {subsection}{\numberline {5.1}Deep Q-Learning Agent}{24}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Imitation-based RL Agent}{28}{subsection.5.2}%
\contentsline {section}{\numberline {6}Conclusion}{30}{section.6}%
\contentsline {subsection}{\numberline {6.1}Deep Q-Learning Agent}{30}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Imitation-based RL Agent}{30}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}Outlook}{30}{subsection.6.3}%
\contentsline {subsubsection}{\numberline {6.3.1}Deep Q-Learning Agent}{30}{subsubsection.6.3.1}%
\contentsline {subsubsection}{\numberline {6.3.2}Imitation-based RL Agent}{31}{subsubsection.6.3.2}%
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
