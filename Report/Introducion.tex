
\fancyhead[R]{Berkay GÃ¼nes}
%- problem\\
%- brief overview of methods:\\
%- Deep Q Learning give examples where its used (e.g. Mario)\\
%- definitely all methods mentioned in the lecture\\
%- maybe research more possibilities and examples when its used

Building games with artificial agents that learn how to play is a long-standing goal in Game-AI. In this work we focus on building an agent that learns to master the classical arcade game Bomberman with the help of reinforcment learning.\\ \\
In the game Bomberman each agents finds itself in a grid of crates and walls and tries to win the game by collecting as many points as possible in each round.\\
He can achieve this by planting bombs to destroy crates and collect coins or kill other agents. \\
In a game like this, with a lot of options and strategies, it gets rather complicated to play the game and is therefore ideally suited to try solving the game by machine learning.\\ \\
The reinforcement learning (RL) approach, used in this project, is ideally suited for teaching games to AIs as it gives the creator the opportunity to guide the agent into the desired direction by creating rewards and punishments for certain behaviours.\\ \\
In this project we mainly focused on the Q-learning algorithm which learns the value of an action in a particular state and has been proven successful in endeavours like this many times. \\
We further extended our model to a Deep Q-Network, which contains a Neural network to process the feature state of the agent. This gives the benefit of faster computation in an environment with a large feature state, as the classical Q-tables scales exponentially to the given input.\\ \\
The code to our agents can be found on Github: \\
\url{https://github.com/bGuenes/bomberman_MLE_homework} \\ \\
Our model handed in to the tournament is called "agent the destroyer of worlds 2" and is a Neural Network with three convolutions and one linear input layer in parallel, followed by four parallel linear inputs, which then merge together into one fully connected layer and then go to the output layer. The structure of the Network is explained in more detail in section \ref{NetworkS}. \\
The agent "destroyer of students" is based on the same network structure, but with a vision field of 9x9 instead of 7x7.\\
In test before the deadline the agent with the smaller vision field performed best and was therefore handed in.\\
The remaining agents in the repository are predecessors with a simpler Network structure containing just one hidden linear layer.

\newpage
\fancyhead[R]{Leonie Boland}
\subsection{Related Work}

- deep q learning in RL growth, applicable in various domains, video games, robotics, decision making processes\\
- basis for deep q learning in paper that plays 7 atari 2600 games with deep q learning, with outstanding results, three even surpassing human expert \\% https://arxiv.org/abs/1312.5602
- most popular ai game is chess, AlphaZero was developed based on AlphaGo Zero, gets more generic, AlphaGo Zero already works with deep neural networks and a tabula rasa reinforcement learning algorithm as well as Monte Carlo Tree Search algortihm for the rules to use it for more games than just go, namely chess and shogi \\% https://arxiv.org/pdf/1712.01815.pdf  %https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf
- hybrid approach with dql is promising in different sytle games
- propose an improvement to Q-Learning because of flaw of overestimating action values, namely double q-learning \\%https://arxiv.org/pdf/1509.06461.pdf
- this is then used for training a mario playing agent \\%https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html
- ppo algorithms alternate between sampling data through interaction with the environment and optimizing an objective function, enabling multiple epochs of minibatch updates, using stochastic gradient ascent, experiments not only simulated robotic locomotion but also atari game, successfully\\ %https://arxiv.org/abs/1707.06347
- bomberman for rl has been studied before as well, imitation-based learner improving with actor-critic (value-based and policy-based) proximal policy optimization method, compared to only ppo with either multi-layer perceptron or a long short term memory network\\%https://inria.hal.science/hal-03652029/document