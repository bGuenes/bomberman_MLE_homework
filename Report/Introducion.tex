\fancyhead[R]{Leonie Boland}
%- problem\\
%- brief overview of methods:\\
%- Deep Q Learning give examples where its used (e.g. Mario)\\
%- definitely all methods mentioned in the lecture\\
%- maybe research more possibilities and examples when its used

Building games with artificial agents that learn how to play is a long-standing goal in Game-AI. In this work we focus on building an agent that learns to master the classical arcade game Bomberman with the help of reinforcment learning.\\ \\
In the game Bomberman each agents finds itself in a grid of crates and walls and tries to win the game by collecting as many points as possible in each round.\\
He can achieve this by planting bombs to destroy crates and collect coins or kill other agents. \\
In a game like this, with a lot of options and strategies, it gets rather complicated to play the game and is therefore ideally suited to try solving the game by machine learning.\\ \\
The reinforcement learning (RL) approach, used in this project, is ideally suited for teaching games to AIs as it gives the creator the opportunity to guide the agent into the desired direction by creating rewards and punishments for certain behaviours.\\ \\
In this project we mainly focused on the Q-learning algorithm which learns the value of an action in a particular state and has been proven successful in endeavours like this many times. \\
We further extended our model to a Deep Q-Network, which contains a Neural network to process the feature state of the agent. This gives the benefit of faster computation in an environment with a large feature state, as the classical Q-tables scales exponentially to the given input.\\ \\
The code to our agents can be found on Github: \\
\url{https://github.com/bGuenes/bomberman_MLE_homework} \\ \\
Our model handed in to the tournament is called ``agent the destroyer of worlds 2'' and is a Neural Network with three convolutions and one linear input layer in parallel, followed by four parallel linear inputs, which then merge together into one fully connected layer and then go to the output layer. The structure of the Network is explained in more detail in section \ref{NetworkS}. \\
The agent ``destroyer of students'' is based on the same network structure, but with a vision field of 9x9 instead of 7x7.\\

\noindent Then we also worked on a second project that follows a slightly different approach than our main project. In the second project, 
the goal is for the agent to first learn from the rule-based agent using classification, and then the agent should be further improved 
using deep Q-learning. In the following, we will refer to the agent from our final project that we 
submitted as the ``Deep Q-Learning Agent'', and the agent from our second project will be called the ``Imitation-based RL Agent''.
You can find our second project in the Github repo called ``xsecond\_agent''.\\

\noindent In test before the deadline the agent with the smaller vision field performed best and was therefore handed in.
The remaining agents in the repository are predecessors with a simpler Network structure containing just one hidden linear layer.

\newpage
\fancyhead[R]{Leonie Boland}
\subsection{Related Work}

In recent years Deep Q-Learning gained popularity in the field of reinforcement learning for applications in various domains, like video games, robotics and decision making processes. The foundation for Deep Q-Learning was established in the paper 'Playing Atari with Deep Reinforcement Learning' \cite{deepRL}. The authors developed the first successful variant of Q-learning in combination with a convolutional neural network and tested it on seven Atari 2600 games from the Arcade Learning Environment.

Thenceforth the algorithm was further developed in the claim to surpassing human performance in more and more games. Van Hasselt et al. \cite{doubleQ} propose an improvement to Deep Q-learning, called Double Q-learning, due to some weaknesses of the Deep Q-learning algorithm, like overestimating action values. This method was used successfully to train a Mario playing agent using Pytorch \cite{mario}.

Another reinforcement learning technique is Proximal Policy Optimization (PPO) \cite{ppo}. PPO algorithms use trust region constrains and clipped objective functions, that allows multiple epochs of minibatch updates. Advantages of this reinforcement learning method are its stability and sample efficiency, which lead to outmatching other online policy gradient methods in simulated robotic locomotion and Atari games. PPO has been applied on Bomberman in two different variations by Goulart et al. \cite{ppobomberman}. This work compared two Bomberman's agent performances after training one with an imitation-based learner improving with actor-critic PPO and the other only using PPO including either a Multi-Layer Perceptron or a Long Short Term-Memory network. Their conclusion is, that the imitation-based agent operates best. 