% Festlegung des Allgemeinen Dokumentenformats
\documentclass[a4paper,12pt,headsepline, fleqn, english]{scrartcl}%article

% Umlaute unter UTF8 nutzen
\usepackage[utf8]{inputenc}

% Variablen
%\input{latex_einstellungen/variablen}

% weitere Pakete
% Grafiken aus PNG Dateien einbinden
\usepackage{graphicx}

% Deutsche Sonderzeichen und Silbentrennung nutzen
%\usepackage[ngerman]{babel}

% Eurozeichen einbinden
\usepackage[right]{eurosym}

% Zeichenencoding
\usepackage[T1]{fontenc}

\usepackage{lmodern}

\usepackage{algpseudocode}
\usepackage{algorithm}
\let\oldReturn\Return
\renewcommand{\Return}{\State\oldReturn}

\usepackage{float}
% floatende Bilder ermöglichen
%\usepackage{floatflt}
\usepackage[font=footnotesize]{caption}
%\usepackage{caption}
\usepackage{subcaption}

% mehrseitige Tabellen ermöglichen
\usepackage{longtable}

% Unterstützung für Schriftarten
%\newcommand{\changefont}[3]{ 
	%\fontfamily{#1} \fontseries{#2} \fontshape{#3} \selectfont}

% Packet für Seitenrandabständex und Einstellung für Seitenränder
\usepackage{geometry}
\geometry{left=3.5cm, right=2cm, top=2.5cm, bottom=2.5cm}

% Paket für Boxen im Text
\usepackage{fancybox}

% bricht lange URLs "schön" um
\usepackage[hyphens,obeyspaces,spaces]{url}

% Paket für Textfarben
\usepackage{color}

\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

% Mathematische Symbole importieren
\usepackage{amssymb}
\usepackage{amsmath}
% auf jeder Seite eine Überschrift (alt, zentriert)
%\pagestyle{headings}

% erzeugt Inhaltsverzeichnis mit Querverweisen zu den Abschnitten (PDF Version)
%\usepackage[bookmarksnumbered,pdftitle={\titleDocument},hyperfootnotes=false]{hyperref}
%\hypersetup{colorlinks, citecolor=red, linkcolor=blue, urlcolor=black}
%\hypersetup{colorlinks, citecolor=black, linkcolor= black, urlcolor=black}

% neue Kopfzeilen mit fancypaket
\usepackage{fancyhdr} %Paket laden
\pagestyle{fancy} %eigener Seitenstil
\fancyhf{} %alle Kopf- und Fußzeilenfelder bereinigen
\fancyhead[L]{\nouppercase{\leftmark}} %Kopfzeile links
\fancyhead[C]{} %zentrierte Kopfzeile
\fancyhead[R]{Leonie Boland} %Kopfzeile rechts
\renewcommand{\headrulewidth}{0.4pt} %obere Trennlinie
\fancyfoot[C]{\thepage} %Seitennummer
%\renewcommand{\footrulewidth}{0.4pt} %untere Trennlinie

%authoryear-comp
\RequirePackage[style = numeric, backend = biber]{biblatex}
\addbibresource{SeminarMML.bib}

% Paket für Zeilenabstand
\usepackage{setspace}



\begin{document}
	\input{Cover}
	
	% römische Numerierung
	\pagenumbering{roman}
	
	% Neue Seite für die Erklärung
	\newpage
	
	\section{Declaration}
	
	\newpage
	
	% Seitenzählung bei Inhaltsverzeichnis beginnen
	\setcounter{page}{1}
	
	% Inhaltsverzeichnis anzeigen
	\thispagestyle{empty}
	\tableofcontents
	
	\newpage
	
	% arabische Seitennummerierung ab hier
	\pagenumbering{arabic}
	
	\section{Introduction}

	The integration of artificial intelligence (AI) into the realm of video gaming has led to groundbreaking advancements 
	in gameplay and immersive experiences. One of the most intriguing challenges in this endeavor is the development of AI 
	agents capable of mastering complex and dynamic games such as Bomberman. Bomberman, a classic arcade-style game, 
	presents a rich and multifaceted environment with intricate decision-making, spatial reasoning, and strategic planning. 
	Solving Bomberman with an AI agent using reinforcement learning techniques is a fascinating and intricate problem, one 
	that holds significant promise for the AI and gaming communities.\\

	\noindent Reinforcement learning (RL), a subfield of machine learning, offers a compelling approach to tackle 
	the Bomberman challenge. RL revolves around training agents to learn optimal strategies by interacting with their 
	environment, taking actions, and receiving feedback in the form of rewards or penalties. In the context of Bomberman, 
	RL techniques can be instrumental in enabling AI agents to navigate mazes, strategically place bombs, avoid traps, and 
	outsmart opponents. Here are a few noteworthy reinforcement learning techniques that hold potential in solving the Bomberman game:

	\begin{description}
	\item[Q-Learning:] Q-learning is a classic RL algorithm that could be applied to Bomberman. 
	It enables agents to learn a value function that maps state-action pairs to expected 
	cumulative rewards. By exploring different actions in the game and updating Q-values, 
	an AI agent can eventually converge on an optimal strategy.

	\item[Deep Q-Networks (DQN):] DQN extends Q-learning by employing deep neural networks 
	to approximate the Q-value function. This technique has been successful in handling 
	complex and high-dimensional state spaces, making it a strong candidate for Bomberman.

	\item[Policy Gradient Methods:] Policy gradient methods aim to directly learn a policy 
	that specifies the agent's actions in different game states. This approach can be 
	effective in scenarios where the optimal policy is not easily represented by a value function, 
	as it allows for a more flexible and direct mapping from states to actions.

	\item[Proximal Policy Optimization (PPO):] PPO is a state-of-the-art RL algorithm that focuses 
	on optimizing policy functions. It offers stability and strong performance in challenging 
	environments, making it suitable for Bomberman's dynamic and adversarial setting.

	\item[Actor-Critic Methods:] Actor-critic methods combine value-based and policy-based approaches, 
	leveraging both a critic (value function) and an actor (policy). This combination can provide the 
	agent with a more robust learning framework, enhancing its decision-making capabilities.

	In the pursuit of solving Bomberman with an AI agent, the selection and fine-tuning 
	of the appropriate RL technique, as well as the integration of domain-specific features, will 
	play pivotal roles in achieving success. This endeavor not only serves as a compelling testbed for 
	reinforcement learning but also has the potential to elevate the overall gaming experience 
	by delivering AI opponents that are challenging, adaptive, and engaging.
	\end{description}
	
	\section{Methods}
	
	In this section, we elucidate our methodology and approaches concerning the various machine learning models 
	that we have developed for the Bomberman game agent. We will also substantiate what has proven effective, articulate 
	the concepts we have discarded, and expound upon the rationale for ultimately selecting the model we have submitted.

	\subsection{Our final best model (first project)}
	
	Explained by Berkay

	\subsection{Our second best model (second project)}
	dwedwedw
	dwdwdwd

	\subsection{Other aproaches that we had}
	frfrf
	rfrfr

	\subsubsection{Q-Tables}
	swsws
	cxwcw
	
	\subsubsection{Coin-Collector Agent that sees only one coin}
	fsdff
	wfwfw
	
	\section{Training}
	
	\input{ExperimentsAndResults}
	
	\section{Conclusion}
	
	\newpage	
	
	%\nocite{*}
	\printbibliography
\end{document}