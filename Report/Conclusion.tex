\fancyhead[R]{Leonie Boland}

\subsection{Deep Q-Learning Agent}

In summary, the approach of using Deep Q-Learning to play Bomberman with reinforcement learning was successful. Once the main framework was in place, we found that it is very important to give a lot of thought into the rewarding and auxiliary rewards as the performance of the agent depends crucially upon that. Certainly, fine tuning hyperparameters also plays an important role, but as soon as the model was running well, there seemed to be no big differences in the performance based on hyperparameter changes.

The model does not provide a perfect performance as explored in the end of section \ref{sec:deepqexperiments}. With the given time constraints we are only able to give hypotheses for the reasons of the malfunctions. Our believe is that, most of the deficits might resolve when working out a better way to implement the state to feature function or the auxiliary rewards.
