@misc{Img:autoencoder,
  author = {COMP THREE INC},
  title = {Variational Autoencoders are Beautiful},
  year = {2019},
  note = {\url{https://www.compthree.com/blog/autoencoder/}}
}

@online{batchnorm,
	ALTeditor = {Towards Data Science},
	author = {Huber, Johann},
	title = {Batch normalization in 3 levels of understanding},
  date = {2020-11},
	url = {https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338#3164},
  urldate = {2023-09}
}

@online{batchnormpos,
	ALTeditor = {Torch},
	author = {Gross, Sam and Wilber, Michael},
	title = {Training and investigating Residual Nets},
        date = {2016-02},
	url = {http://torch.ch/blog/2016/02/04/resnets.html},
        urldate = {2023-09}
}

@online{selfplay,
	ALTeditor = {Hugging Face},
	author = {Simonini, Thomas},
	title = {Self-Play: a classic technique to train competitive agents in adversarial games},
	url = {https://huggingface.co/learn/deep-rl-course/unit7/self-play},
        urldate = {2023-09}
}

@online{Art:torchQlearn,
  ALTeditor = {Towards Data Science},
  author = {Pytorch},
  title = {REINFORCEMENT LEARNING (DQN) TUTORIAL},
  year = {2023},
  url = {https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html}
}

@online{Onl:greedy,
  ALTeditor = {Towards Data Science},
  author = {paperswithcode},
  title = {Epsilon Greedy Exploration},
  year = {2019},
  url = {https://paperswithcode.com/method/epsilon-greedy-exploration}
}

@online{Onl:pytorch,
  ALTeditor = {Towards Data Science},
  author = {Pytorch},
  title = {Pytorch},
  year = {2023},
  url = {https://pytorch.org/}
}

@online{Onl:replaybuff,
  ALTeditor = {Towards Data Science},
  author = {paperswithcode},
  title = {Experience Replay},
  year = {2017},
  url = {https://paperswithcode.com/method/experience-replay}
}

@online{Onl:dataAug,
  ALTeditor = {Towards Data Science},
  author = {saturncloud},
  title = {Data Augmentation in PyTorch: A Comprehensive Guide for Data Scientists},
  year = {2023},
  url = {https://saturncloud.io/blog/data-augmentation-in-pytorch-a-comprehensive-guide-for-data-scientists/#:~:text=Data%20augmentation%20is%20a%20technique,or%20changing%20the%20color%20scheme.}
}

@article{Art:Overfitting,
doi = {10.1088/1742-6596/1168/2/022022},
url = {https://dx.doi.org/10.1088/1742-6596/1168/2/022022},
year = {2019},
month = {2},
publisher = {IOP Publishing},
volume = {1168},
number = {2},
pages = {022022},
author = {Xue Ying},
title = {An Overview of Overfitting and its Solutions},
journal = {Journal of Physics: Conference Series}
}

@online{Onl:autoencoder,
  ALTeditor = {Towards Data Science},
  author = {Elias Saalmann},
  title = {Einführung in Autoencoder und Convolutional Neural Networks},
  year = {2018},
  url = {https://dbs.uni-leipzig.de/file/Saalmann_Ausarbeitung.pdf}
}

@online{Onl:crossentr,
  ALTeditor = {Towards Data Science},
  author = {educative},
  title = {What is cross-entropy loss in PyTorch?},
  year = {2023},
  url = {https://www.educative.io/answers/what-is-cross-entropy-loss-in-pytorch}
}

@online{Onl:qtabledis,
  ALTeditor = {Towards Data Science},
  author = {baeldung},
  title = {Q-Learning vs. Deep Q-Learning},
  year = {2023},
  url = {https://www.baeldung.com/cs/q-learning-vs-deep-q-learning-vs-deep-q-network#:~:text=One%20of%20the%20main%20drawbacks,to%20store%20the%20Q%2Dvalues.}
}

@article{deepRL,
    doi = {arXiv:1312.5602},
    url = {https://arxiv.org/pdf/1312.5602.pdf},
    author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
    title = {Playing Atari with Deep Reinforcement Learning},
    publisher = {arXiv},
    year = {2013}
}

@article{doubleQ,
    doi = {arXiv:1509.06461},
    url = {https://arxiv.org/pdf/1509.06461.pdf},
    author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
    title = {Deep Reinforcement Learning with Double Q-learning},
    publisher = {arXiv},
    year = {2015}
}

@online{mario,
  ALTeditor = {PyTorch},
  author = {Feng, Yuansong and Subramanian, Suraj and Wang, Howard and Guo, Steven},
  title = {Train a Mario-Playing RL Agent},
  year = {2023},
  url = {https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html}
}

@article{ppo,
    doi = {arXiv:1707.06347},
    url = {https://arxiv.org/pdf/1707.06347.pdf},
    author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
    title = {Proximal Policy Optimization Algorithms},
    publisher = {arXiv},
    year = {2017}
}

@article{ppobomberman,
    doi = {hal-03652029},
    url = {https://inria.hal.science/hal-03652029/document},
    author = {Goulart, Ícaro and Paes, Aline and Clua, Esteban},
    title = {Learning How to Play Bomberman with Deep
Reinforcement and Imitation Learning},
    publisher = {HAL open ouverte},
    year = {2022}
}